

## âš™ï¸ **Installation (facile, promis ğŸ¤)**  

### ğŸ“Œ **Ce quâ€™il te faut**  

Avant de commencer, assure-toi dâ€™avoir :  
âœ”ï¸ **Python 3.8+** installÃ© (prends la derniÃ¨re version dispo, câ€™est toujours mieux)  
âœ”ï¸ **Ollama** (le moteur qui va faire tourner lâ€™IA)  

### ğŸ”½ **Installer Ollama**  

1. TÃ©lÃ©charge et installe **Ollama** depuis leur site :  
   ğŸ‘‰ [https://ollama.ai/download](https://ollama.ai/download)  

2. Lance Ollama avec cette commande (Ã§a dÃ©marre le serveur) :  

   ```bash
   ollama serve
   ```

---

## ğŸ—ï¸ **Mise en place du projet**  

### ğŸ”¹ **1. Clone le repo et installe les dÃ©pendances**  

Si ce nâ€™est pas dÃ©jÃ  fait, clone le projet et rentre dedans :  

```bash
git clone https://github.com/camcarre/fils-rouge.git

```

Active un environnement virtuel pour Ã©viter dâ€™installer des paquets nâ€™importe oÃ¹ :  

```bash
python -m venv venv
source venv/bin/activate  # macOS/Linux
# ou pour Windows :
venv\Scripts\activate
```

Installe les dÃ©pendances :  

```bash
pip install -r requirements.txt
```

### ğŸ”¹ **2. Configurer les paramÃ¨tres**  

Copie le fichier de config et ajuste-le si besoin :  

```bash
cp config.py.example config.py
```

Tu peux modifier **config.py** pour changer le modÃ¨le utilisÃ©.  

### ğŸ”¹ **3. Changer de modÃ¨le IA (si tu veux)**  

Si tu veux tester un autre modÃ¨le, fais comme Ã§a :  

1. Stoppe Ollama :  

   ```bash
   ollama stop
   ```

2. TÃ©lÃ©charge un modÃ¨le (exemple avec **Mistral**) :  

   ```bash
   ollama pull mistral:latest
   ```

3. Mets Ã  jour ton fichier **config.py** :  

   ```python
   IA_MODEL = "mistral:latest"
   ```

4. RedÃ©marre Ollama :  

   ```bash
   ollama serve
   ```

---

## ğŸš€ **Lancer lâ€™application**  

1. Assure-toi quâ€™Ollama tourne bien en arriÃ¨re-plan :  

   ```bash
   ollama serve
   ```

2. DÃ©marre le chatbot :  

   ```bash
   python main.py
   ```

3. Ouvre ton navigateur et va sur **[http://127.0.0.1:5000](http://127.0.0.1:5000)**  

---

## ğŸ” **Voir les modÃ¨les disponibles**  

Tu peux voir quels modÃ¨les sont installÃ©s sur ta machine avec :  

```bash
ollama list
```

Si tu veux tester un autre modÃ¨le :  

```bash
ollama pull nom_du_modele
```
La liste complÃ¨te est dispo ici : **[https://ollama.ai/models](https://ollama.ai/models)**  

---

## ğŸ› ï¸ **Besoin dâ€™aide ?**  

Si tâ€™as un souci ou une question, check dâ€™abord la doc officielle dâ€™Ollama :  
ğŸ‘‰ [https://ollama.ai/docs](https://ollama.ai/docs)  
